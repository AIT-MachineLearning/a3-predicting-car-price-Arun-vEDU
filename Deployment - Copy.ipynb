{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv('C:\\\\Git\\\\a3-predicting-car-price-Arun-vEDU\\\\Cars_a3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Prepare data\n",
    "# y is simply the selling price colomn\n",
    "y = df[\"selling_price\"]\n",
    "\n",
    "# Covert into log scale\n",
    "y_log = np.log(df[\"selling_price\"])\n",
    "\n",
    "\n",
    "# Using pd.cut to bin data into 4 classes\n",
    "binned_data = pd.cut(y_log , bins=4) #now our y is four classes thus require multinomial\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "# y is simply the selling price colomn\n",
    "y = df[\"selling_price\"]\n",
    "\n",
    "# Covert into log scale\n",
    "y_log = np.log(df[\"selling_price\"])\n",
    "\n",
    "\n",
    "binned_data = pd.cut(y_log, bins=4, labels=False)  # Creates 0, 1, 2, 3\n",
    "# plot the values\n",
    "# Value counts for each bin\n",
    "bin_counts = pd.value_counts(binned_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add binned_data as a new column in the DataFrame\n",
    "df['binned_price'] = binned_data\n",
    "\n",
    "# Step 3: Prepare features and labels\n",
    "X = df.drop(columns=['selling_price', 'binned_price']) \n",
    "y = df['binned_price']  # Original binned labels (0, 1, 2, 3)\n",
    "\n",
    "#print(binned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my selected features are 'Max Power' and 'Year' from a1 and a2 assignments.\n",
    "\n",
    "# Split the 'max_power' column by space\n",
    "df[['max_power', 'max_power_unit']] = df['max_power'].str.split(' ', expand=True)\n",
    "\n",
    "# Remove the 'max_power_unit' column\n",
    "df = df.drop('max_power_unit', axis=1)\n",
    "df.columns\n",
    "\n",
    "# Convert 'max_power' to a numeric type\n",
    "df['max_power'] = pd.to_numeric(df['max_power'])\n",
    "\n",
    "# To check the data types of the columns\n",
    "#print(df)\n",
    "#print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is my selected strong features\n",
    "# I have selected 2 features \"year\" and \"max_power\"\n",
    "# less features are better.\n",
    "X = df[        ['year', 'max_power']        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into train and test\n",
    "# test set is 20% of our dataset.\n",
    "# This is a medium size data set, that is why it is selected 80%  traning set and 20% test set \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#  X is your feature set and binned_data is your target (with 4 classes)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year          0\n",
       "max_power    49\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null values in traning set,The .isna() method is used to detect missing values (not avalible) and count it\n",
    "X_train[['year', 'max_power']].isna().sum()\n",
    "#check for null values in test set, and count it\n",
    "X_test[['year', 'max_power']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# max power has 167 missing values. So we can fill them with ratio, the distribution of the 'max_power' is skewed.\n",
    "# X_train is  training dataset and 'max_power' contains missing values\n",
    "\n",
    "# Step 1: Calculate the ratio of each unique value in 'max_power'\n",
    "ratio = X_train['max_power'].value_counts(normalize=True)\n",
    "\n",
    "# Step 2: Define a function to sample values based on these ratios\n",
    "def fill_missing_values(column, value_distribution):\n",
    "    missing_indices = column[column.isnull()].index  # Find indices where values are missing\n",
    "    filled_values = np.random.choice(value_distribution.index, \n",
    "                                     size=len(missing_indices), \n",
    "                                     p=value_distribution.values)  # Sample from distribution\n",
    "    \n",
    "    # Fill the missing values with sampled values\n",
    "    column.loc[missing_indices] = filled_values\n",
    "    return column\n",
    "\n",
    "# Step 3: Fill missing values in 'max_power' based on the ratio\n",
    "X_train['max_power'] = fill_missing_values(X_train['max_power'], ratio)\n",
    "\n",
    "# Now, X_train['max_power'] has the missing values filled based on the value distribution\n",
    "print(X_train['max_power'].isnull().sum())  # This should return 0 (no missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Fill missing values in 'max_power' in X_test using the same ratio as X_train\n",
    "X_test['max_power'] = fill_missing_values(X_test['max_power'], ratio)\n",
    "\n",
    "# Now, X_test['max_power'] should have the missing values filled based on the distribution from X_train\n",
    "print(X_test['max_power'].isnull().sum())  # This should also return 0 (no missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After SMOTE: Counter({1: 3443, 0: 3443, 2: 3443, 3: 3443})\n",
      "Loss at iteration 0: 1.3946512070852248\n",
      "Loss at iteration 500: 0.5009271878102229\n",
      "Loss at iteration 1000: 0.4605685508039379\n",
      "Loss at iteration 1500: 0.443768207172264\n",
      "Loss at iteration 2000: 0.43472778916148125\n",
      "Loss at iteration 2500: 0.4291517357313967\n",
      "Loss at iteration 3000: 0.42539110314360723\n",
      "Loss at iteration 3500: 0.42268769071606244\n",
      "Loss at iteration 4000: 0.4206499049236603\n",
      "Loss at iteration 4500: 0.4190575691305642\n",
      "Time taken: 7.98458194732666\n",
      "Macro f1: 0.6874932479024042\n"
     ]
    }
   ],
   "source": [
    "# I'm running the custom class for evaluate bast parameters with out mlflow. \n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Step 1: Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Scaled X_train\n",
    "X_test_scaled = scaler.transform(X_test)        # Scaled X_test\n",
    "\n",
    "y_labels = y_train  # Assuming y_train contains the labels (0, 1, 2, 3)\n",
    "\n",
    "# Step 2: Apply SMOTE to the scaled X_train\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_labels_smote = smote.fit_resample(X_train_scaled, y_labels)  # Use the scaled X_train\n",
    "\n",
    "# Check the class distribution after SMOTE\n",
    "print(\"After SMOTE:\", Counter(y_labels_smote))\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_labels_smote_one_hot = pd.get_dummies(y_labels_smote, prefix='class')\n",
    "\n",
    "# Step 3: Add intercept term AFTER SMOTE and scaling\n",
    "intercept_train = np.ones((X_train_smote.shape[0], 1))  # Shape (m, 1)\n",
    "X_train_with_intercept = np.concatenate((intercept_train, X_train_smote), axis=1)  # Shape (m, n + 1)\n",
    "\n",
    "intercept_test = np.ones((X_test_scaled.shape[0], 1))  # Shape (m, 1)\n",
    "X_test_with_intercept = np.concatenate((intercept_test, X_test_scaled), axis=1)  # Shape (m, n + 1)\n",
    "\n",
    "# Logistic Regression class\n",
    "class LogisticRegression:\n",
    "    def __init__(self, k, n, method, alpha=0.1, max_iter=5000, use_penalty=False, penalty='ridge', lambda_=0.01):\n",
    "        self.k = k  # Number of classes\n",
    "        self.n = n  # Number of features\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.max_iter = max_iter  # Maximum iterations\n",
    "        self.method = method  # Optimization method: 'batch', 'minibatch', or 'sto'\n",
    "        self.use_penalty = use_penalty  # Whether to use penalty (regularization)\n",
    "        self.penalty = penalty  # Type of penalty ('ridge' for L2)\n",
    "        self.lambda_ = lambda_  # Regularization strength\n",
    "        self.W = np.random.rand(n + 1, k)  # Initialize weights\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.W = np.random.rand(self.n + 1, self.k)  # Initialize weights\n",
    "        self.losses = []  # To store loss values\n",
    "        \n",
    "        if self.method == \"batch\":\n",
    "            start_time = time.time()\n",
    "            for i in range(self.max_iter):\n",
    "                loss, grad = self.gradient(X, Y)\n",
    "                self.losses.append(loss)\n",
    "                self.W -= self.alpha * grad  # Update weights\n",
    "                if i % 500 == 0:\n",
    "                    print(f\"Loss at iteration {i}: {loss}\")\n",
    "            print(f\"Time taken: {time.time() - start_time}\")\n",
    "        \n",
    "        elif self.method == \"minibatch\":\n",
    "            start_time = time.time()\n",
    "            batch_size = int(0.3 * X.shape[0])\n",
    "            for i in range(self.max_iter):\n",
    "                indices = np.random.choice(X.shape[0], size=batch_size, replace=False)  # Randomly select indices for the batch\n",
    "                batch_X = X[indices]\n",
    "                batch_Y = Y[indices]\n",
    "                loss, grad = self.gradient(batch_X, batch_Y)\n",
    "                self.losses.append(loss)\n",
    "                self.W -= self.alpha * grad  # Update weights\n",
    "                if i % 500 == 0:\n",
    "                    print(f\"Loss at iteration {i}: {loss}\")\n",
    "            print(f\"Time taken: {time.time() - start_time}\")\n",
    "        \n",
    "        elif self.method == \"sto\":\n",
    "            start_time = time.time()\n",
    "            for i in range(self.max_iter):\n",
    "                idx = np.random.randint(X.shape[0])  # Randomly select an index\n",
    "                X_train = X[idx, :].reshape(1, -1)  # Reshape for a single sample\n",
    "                Y_train = Y[idx].reshape(1, -1)  # Reshape for a single sample\n",
    "                loss, grad = self.gradient(X_train, Y_train)\n",
    "                self.losses.append(loss)\n",
    "                self.W -= self.alpha * grad  # Update weights\n",
    "                if i % 500 == 0:\n",
    "                    print(f\"Loss at iteration {i}: {loss}\")\n",
    "            print(f\"Time taken: {time.time() - start_time}\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Method must be one of the following: \"batch\", \"minibatch\", or \"sto\".')\n",
    "        \n",
    "    def gradient(self, X, Y):\n",
    "        m = X.shape[0]  # Number of training examples\n",
    "        h = self.h_theta(X, self.W)  # Hypothesis\n",
    "        loss = -np.sum(Y * np.log(h)) / m  # Cross-entropy loss\n",
    "        \n",
    "        # Apply penalty if use_penalty is True\n",
    "        if self.use_penalty and self.penalty == 'ridge':\n",
    "            loss += (self.lambda_ / (2 * m)) * np.sum(np.square(self.W))  # Ridge penalty (L2)\n",
    "        \n",
    "        error = h - Y  # Error term\n",
    "        grad = self.softmax_grad(X, error)\n",
    "        \n",
    "        # Apply gradient for penalty if use_penalty is True\n",
    "        if self.use_penalty and self.penalty == 'ridge':\n",
    "            grad += (self.lambda_ / m) * self.W  # Add Ridge gradient (L2)\n",
    "        \n",
    "        return loss, grad\n",
    "        \n",
    "    def softmax(self, theta_t_x):\n",
    "        # Ensure input is a NumPy array\n",
    "        theta_t_x = np.array(theta_t_x)\n",
    "    \n",
    "        # Perform softmax calculation\n",
    "        exp_values = np.exp(theta_t_x - np.max(theta_t_x, axis=1, keepdims=True))\n",
    "        return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "    def softmax_grad(self, X, error):\n",
    "        return X.T @ error / X.shape[0]\n",
    "\n",
    "    def h_theta(self, X, W):\n",
    "        return self.softmax(X @ W)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "            h = self.h_theta(X_test, self.W)  # Get the probabilities\n",
    "            return np.argmax(h, axis=1)  # Return the predicted class labels directly\n",
    "\n",
    "    def f1_score(self, y_true, y_pred, class_label):\n",
    "        prec = self.precision(y_true, y_pred, class_label)\n",
    "        rec = self.recall(y_true, y_pred, class_label)\n",
    "        return 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0\n",
    "\n",
    "    def precision(self, y_true, y_pred, class_label):\n",
    "        TP = np.sum((y_true == class_label) & (y_pred == class_label))\n",
    "        FP = np.sum((y_true != class_label) & (y_pred == class_label))\n",
    "        return TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "    def recall(self, y_true, y_pred, class_label):\n",
    "        TP = np.sum((y_true == class_label) & (y_pred == class_label))\n",
    "        FN = np.sum((y_true == class_label) & (y_pred != class_label))\n",
    "        return TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    def macro_f1(self, y_true, y_pred):\n",
    "        classes = np.unique(y_true)\n",
    "        f1_scores = [self.f1_score(y_true, y_pred, class_label) for class_label in classes]\n",
    "        return np.mean(f1_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
