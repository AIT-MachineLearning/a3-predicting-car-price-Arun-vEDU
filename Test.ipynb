{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv('C:\\\\Git\\\\a3-predicting-car-price-Arun-vEDU\\\\Cars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Prepare data\n",
    "# y is simply the selling price colomn\n",
    "y = df[\"selling_price\"]\n",
    "\n",
    "# Covert into log scale\n",
    "y_log = np.log(df[\"selling_price\"])\n",
    "\n",
    "\n",
    "# Using pd.cut to bin data into 4 classes\n",
    "binned_data = pd.cut(y_log , bins=4) #now our y is four classes thus require multinomial\n",
    "\n",
    "\n",
    "print(binned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the values\n",
    "# Value counts for each bin\n",
    "bin_counts = pd.value_counts(binned_data)\n",
    "\n",
    "# Bar plot\n",
    "bin_counts.sort_index().plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of Selling Prices (Log Scale Bins)')\n",
    "plt.xlabel('Price Range (Log Scale)')\n",
    "plt.ylabel('Number of Observations')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, k, n, method, alpha=0.01, max_iter=5000, use_penalty=False, penalty='ridge', lambda_=0.01):\n",
    "        self.k = k  # Number of classes\n",
    "        self.n = n  # Number of features\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.max_iter = max_iter  # Maximum iterations\n",
    "        self.method = method  # Optimization method: 'batch', 'minibatch', or 'sto'\n",
    "        self.use_penalty = use_penalty  # Whether to use penalty (regularization)\n",
    "        self.penalty = penalty  # Type of penalty ('ridge' for L2)\n",
    "        self.lambda_ = lambda_  # Regularization strength\n",
    "        self.W = np.random.rand(n + 1, k)  # Initialize weights\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.W = np.random.rand(self.n + 1, self.k)   # Initialize weights\n",
    "        self.losses = []  # To store loss values\n",
    "        \n",
    "        if self.method == \"batch\":\n",
    "            start_time = time.time()\n",
    "            for i in range(self.max_iter):\n",
    "                loss, grad = self.gradient(X, Y)\n",
    "                self.losses.append(loss)\n",
    "                self.W -= self.alpha * grad  # Update weights\n",
    "                if i % 500 == 0:\n",
    "                    print(f\"Loss at iteration {i}: {loss}\")\n",
    "            print(f\"Time taken: {time.time() - start_time}\")\n",
    "        \n",
    "        elif self.method == \"minibatch\":\n",
    "            start_time = time.time()\n",
    "            batch_size = int(0.3 * X.shape[0])\n",
    "            for i in range(self.max_iter):\n",
    "                indices = np.random.choice(X.shape[0], size=batch_size, replace=False)  # Randomly select indices for the batch\n",
    "                batch_X = X[indices]\n",
    "                batch_Y = Y[indices]\n",
    "                loss, grad = self.gradient(batch_X, batch_Y)\n",
    "                self.losses.append(loss)\n",
    "                self.W -= self.alpha * grad  # Update weights\n",
    "                if i % 500 == 0:\n",
    "                    print(f\"Loss at iteration {i}: {loss}\")\n",
    "            print(f\"Time taken: {time.time() - start_time}\")\n",
    "        \n",
    "        elif self.method == \"sto\":\n",
    "            start_time = time.time()\n",
    "            for i in range(self.max_iter):\n",
    "                idx = np.random.randint(X.shape[0])  # Randomly select an index\n",
    "                X_train = X[idx, :].reshape(1, -1)  # Reshape for a single sample\n",
    "                Y_train = Y[idx].reshape(1, -1)  # Reshape for a single sample\n",
    "                loss, grad = self.gradient(X_train, Y_train)\n",
    "                self.losses.append(loss)\n",
    "                self.W -= self.alpha * grad  # Update weights\n",
    "                if i % 500 == 0:\n",
    "                    print(f\"Loss at iteration {i}: {loss}\")\n",
    "            print(f\"Time taken: {time.time() - start_time}\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Method must be one of the following: \"batch\", \"minibatch\", or \"sto\".')\n",
    "        \n",
    "    def gradient(self, X, Y):\n",
    "        m = X.shape[0]  # Number of training examples\n",
    "        h = self.h_theta(X, self.W)  # Hypothesis\n",
    "        loss = -np.sum(Y * np.log(h)) / m  # Cross-entropy loss\n",
    "        \n",
    "        # Apply penalty if use_penalty is True\n",
    "        if self.use_penalty and self.penalty == 'ridge':\n",
    "            loss += (self.lambda_ / (2 * m)) * np.sum(np.square(self.W))  # Ridge penalty (L2)\n",
    "        \n",
    "        error = h - Y  # Error term\n",
    "        grad = self.softmax_grad(X, error)\n",
    "        \n",
    "        # Apply gradient for penalty if use_penalty is True\n",
    "        if self.use_penalty and self.penalty == 'ridge':\n",
    "            grad += (self.lambda_ / m) * self.W  # Add Ridge gradient (L2)\n",
    "        \n",
    "        return loss, grad\n",
    "\n",
    "    def softmax(self, theta_t_x):\n",
    "        return np.exp(theta_t_x - np.max(theta_t_x, axis=1, keepdims=True)) / np.sum(np.exp(theta_t_x - np.max(theta_t_x, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "\n",
    "    def softmax_grad(self, X, error):\n",
    "        return X.T @ error / X.shape[0]\n",
    "\n",
    "\n",
    "    \n",
    "    def h_theta(self, X, W):\n",
    "        #print(f\"X shape: {X.shape}\")\n",
    "        #print(f\"W shape: {W.shape}\")\n",
    "        return self.softmax(X @ W)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return np.argmax(self.h_theta(X_test, self.W), axis=1)\n",
    "    \n",
    "    def plot(self):\n",
    "        plt.plot(np.arange(len(self.losses)), self.losses, label=\"Train Losses\")\n",
    "        plt.title(\"Losses over iterations\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(y_true == y_pred)\n",
    "        total = len(y_true)\n",
    "        return correct / total\n",
    "\n",
    "    def precision(self, y_true, y_pred, class_label):\n",
    "        TP = np.sum((y_true == class_label) & (y_pred == class_label))\n",
    "        FP = np.sum((y_true != class_label) & (y_pred == class_label))\n",
    "        return TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "    def recall(self, y_true, y_pred, class_label):\n",
    "        TP = np.sum((y_true == class_label) & (y_pred == class_label))\n",
    "        FN = np.sum((y_true == class_label) & (y_pred != class_label))\n",
    "        return TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    def f1_score(self, y_true, y_pred, class_label):\n",
    "        prec = self.precision(y_true, y_pred, class_label)\n",
    "        rec = self.recall(y_true, y_pred, class_label)\n",
    "        return 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0\n",
    "    \n",
    "    # New macro precision, recall, and f1 functions\n",
    "    def macro_precision(self, y_true, y_pred):\n",
    "        classes = np.unique(y_true)\n",
    "        precision_scores = [self.precision(y_true, y_pred, class_label) for class_label in classes]\n",
    "        return np.mean(precision_scores)\n",
    "\n",
    "    def macro_recall(self, y_true, y_pred):\n",
    "        classes = np.unique(y_true)\n",
    "        recall_scores = [self.recall(y_true, y_pred, class_label) for class_label in classes]\n",
    "        return np.mean(recall_scores)\n",
    "\n",
    "    def macro_f1(self, y_true, y_pred):\n",
    "        classes = np.unique(y_true)\n",
    "        f1_scores = [self.f1_score(y_true, y_pred, class_label) for class_label in classes]\n",
    "        return np.mean(f1_scores)\n",
    "    \n",
    "    def weighted_recall(self, class_labels, weights, recalls):\n",
    "        weighted_sum = 0\n",
    "        for i in range(len(class_labels)):  # Iterate based on index\n",
    "            class_label = class_labels[i]  # Get class label at index i\n",
    "            weight = weights[i]  # Get weight at index i\n",
    "            weighted_sum += weight * recalls[class_label]  # Multiply recall by weight\n",
    "        return weighted_sum / sum(weights)\n",
    "\n",
    "    def weighted_precision(self, class_labels, weights, precisions):\n",
    "        weighted_sum = 0\n",
    "        for i in range(len(class_labels)):  # Iterate based on index\n",
    "            class_label = class_labels[i]  # Get class label at index i\n",
    "            weight = weights[i]  # Get weight at index i\n",
    "            weighted_sum += weight * precisions[class_label]  # Multiply precision by weight\n",
    "        return weighted_sum / sum(weights)\n",
    "    \n",
    "    def weighted_f1(self, class_labels, weights, precisions, recalls):\n",
    "        weighted_sum = 0\n",
    "        for i in range(len(class_labels)):  # Iterate based on index\n",
    "            class_label = class_labels[i]  # Get class label at index i\n",
    "            weight = weights[i]  # Get weight at index i\n",
    "        \n",
    "            # Calculate the F1 score for the current class\n",
    "            if precisions[class_label] + recalls[class_label] > 0:  # Prevent division by zero\n",
    "                f1_score = 2 * (precisions[class_label] * recalls[class_label]) / (precisions[class_label] + recalls[class_label])\n",
    "            else:\n",
    "                f1_score = 0  # Assign 0 if both precision and recall are 0\n",
    "\n",
    "            weighted_sum += weight * f1_score  # Multiply F1 score by weight\n",
    "        return weighted_sum / sum(weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
